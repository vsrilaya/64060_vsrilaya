---
title: "Final Assignment"
author: "Srilaya Valmeekam"
date: "2023-05-06"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
library(dplyr)
library(caret)
library(factoextra)
library(leaps)
library(dbscan)
library(esquisse)
```
**Data Cleansing:**
```{r}
library(readr)
Data <- read.csv("Downloads/fuelcostpudl .csv")
```

```{r}
#Using NA to replace missing values

Na<- Data %>% replace(.=="",NA)

#Obtaining the percentages of null values in each column
missing_values<- (colMeans(is.na(Na))*100)

#Removing variables with null values and percentages greater than 50%, as well as a few other variables that do not contribute significantly to the analysis

Data_1<- subset(Data,select=-c(1:5,7:8,12:14,22:25,26:30))
```


```{r}
#Random sampling of 2% data
set.seed(2467)
data_2<-sample_n(Data_1,12000)
```


**Data Exploration:**
```{r}
#Creating dummy variables to convert fuel_type_code_pudl into numerical data

fuel_type_coal <- ifelse(data_2$fuel_type_code_pudl=="coal" ,1,0)
fuel_type_gas <- ifelse(data_2$fuel_type_code_pudl=="gas" ,1,0)
fuel_type_oil <- ifelse(data_2$fuel_type_code_pudl=="oil" ,1,0)

#Adding these new columns to the existing dataframe

New_data<- cbind(data_2[,-3],fuel_type_coal,fuel_type_gas,fuel_type_oil)

```

**Data Preparation:**
```{r}
#Splitting data into training and test

Split_data<-createDataPartition(New_data$fuel_received_units,p=.75,list=FALSE)
Training<-New_data[Split_data,]
Test<-New_data[-Split_data,]

Training[is.na(Training)] <- 0
Test[is.na(Test)] <- 0


```

**Modeling Strategy:**

**The first method was to use the k-means algorithm. However, after observing that the k-means clusters produced were overlapped, it became clear that the data contains border points and outliers.Since the variation between the clusters was too small, I did not prefer to continue my analysis with those clusters.*

*The next immediate idea was to perform DBSCAN algorithm as it handles border points and outliers.*
```{r}

#Selection of numerical data for clustering

Training_numerical<-Training[,c(4:9,11:13)]

#Normalizing the data
Training_norm<-scale(Training_numerical)
```


```{r}

dbscan::kNNdistplot(Training_norm, k =  2)
abline(h = 0.5,col="red")

```


*Based on the plot above, set the epsilon value to 0.5. After a few attempts with different values, the option of minPts was made. When minPts was set to 100, I got three perfect clusters with greater variation between them.*
```{r}
db <- dbscan::dbscan(Training_norm, eps = 0.5, minPts = 100)
db
```
*There are 914 boundary points in the data, and three clusters have been formed, with 4732,740, and 2614 data points in each cluster.*

```{r}

 #Plotting clusters for better data visualization:
fviz_cluster(db,Training_numerical,main="3 clusters")

#Assigning clusters to the original data:
assigned_data<-cbind(Training_numerical,db$cluster)


#Finding mean within each cluster to interpret the clusters:
mean_k3 <- Training_numerical %>% mutate(Cluster = db$cluster) %>% group_by(Cluster) %>% summarise_all("mean")
head(mean_k3)
```
**Each cluster's interpretation**

*It can be observed that each of the fuel type falls under each cluster. Therefore, my analysis of each cluster is based on fuel types.*

**Names of each cluster:**

*Cluster 1: Gas*

*Cluster 2: Oil*

*Cluster 3: Coal*


**Interpreting the pattern in the clusters with respect to the Categorical variables:**
```{r}

plots <- Training[,c(1:3,10)] %>% mutate(Clusters=db$cluster)

ggplot(plots, mapping = aes(factor(Clusters), fill =contract_type_code_label))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =energy_source_code_label))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =fuel_group_code))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =primary_transportation_mode_code))+geom_bar(position='dodge')+labs(x ='Clusters')
```



**Analysis of Cluster 1: Gas**

*Gas has the average lowest fuel cost per mmbtu. That also explains why is it supplied the most number of avergae units of fuel.*
*Gas does not contain any ash,sulfur and mercury content which makes it a good type of fuel that can be used.*
*It also contains the lowest average of fuel mmbtu per unit. which means that the heat content generated by fuel is less*
*Based on the graph, most gas type fuel is purchased on spot and a relatively lesser amount is purchased on contract.*
*Energy source code is Natural gas and the most commonly used transportation type to supply this type of fuel is through pipelines(PL).*

**Analysis of Cluster 2: Oil**

*The average cost of oil per mmbtu is 10.49, making it the most expensive type of fuel in the USA.*
*The average units of oil received in comparision to gas and coal is very less, probably because it is the most expensive fuel type.*
*Even oil doesnot contain ash and mercury percentage but do has a little percent of sulfur content.*
*From the graphs, oil is only purchased on spot. No contract based purchases have been recorded.*
*The energy source code for this type of fuel is DFO which means Distillate Fuel Oil which also includes*


**Analysis of Cluster 3: Coal**
*Coal is the least expensive type of fuel and is also widely supplied in the USA.*
*Unlike other two fuels, it contains ash,sulfur and mercury content.*
*The average heat energy received from coal is 21.5612.*
*From the graphs of categorical variables, coal is purchased mostly on spot.*
*The energy source code for this type of fuel is BIT and SUB, which indicates that conventional Steam Coal is supplied the most in the U.S.A*



\newpage

***Extra-Credit:***
```{r}

#Running the multiple linear regression model to determine the best set of variables to predict fuel_cost_per_mmbtu by considering variables which were used to form clusters:

Model<- lm(Training_numerical$fuel_cost_per_mmbtu~.,data=Training_numerical)
summary(Model)

#Fuel received units,fuel_type_coal and fuel_type_oil best determine the fuel_cost_per_mmbtu variable.


#Checking the prediction of the above model on Test data
Test_data<- Test[,c(4:9,11:13)]
Test_Model<-predict(Model, data = Test_data)

#Predicting clusters for Test data:
Test_norm<-scale(Test_data)

Testing_clusters<- predict(db,newdata = Test_norm,data=Training_norm)


#Appending cluster information and above predicted fuel cost per unit values to the test data:
Test_predicted_data<- cbind(Test_data,Test_Model,Testing_clusters)
head(Test_predicted_data)

#Finding out the averages to see how close the predicted values are to the actual fuel_cost values:
mean_Predicted_Test <- Test_predicted_data %>% mutate(Cluster = Testing_clusters) %>% group_by(Cluster) %>% summarise_all("mean")
head(mean_Predicted_Test)

```
*Observations: We could see that the averages of predicted values in each cluster is comparitively closer to the averages of actual fuel cost values. This shows that by choosing variables with significant relationship and cluster information leads to better prediction.*